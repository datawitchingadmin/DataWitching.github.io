
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Transition from SQL to PySpark II</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="Transition_from_SQL_to_PySpark_II"
                  title="Transition from SQL to PySpark II"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Summary" duration="0">
        <p>這篇 Codelab 主要要比較三種不同語法之間的差異，分別是 <strong>SQL</strong> 、pyspark 中的 <strong>spark.sql()</strong> 和 <strong>PySpark</strong> 的方法，要運算出需要的分析結果，可以同時用三種不一樣的方法進行，後面會以同一個例子，用三種方法運算一遍，可以發現三種方法相異和相同之處</p>
<p>在 spark.sql() 中的函數，基本上和 SQL 一模一樣，因此在介紹函數時，主要以 <strong>SQL</strong> 和 <strong>PySpark</strong> 為比較重點，最後會再比較三種方法的完整寫法</p>
<p class="image-container"><img style="width: 601.70px" src="img/d65389fed999fc15.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="SELECT FROM vs read() &#43; df.select()" duration="0">
        <p>在 <strong>SQL</strong> 中，SELECT .. FROM 是用來讀取資料和選擇需要的欄位</p>
<p>在 <strong>PySpark</strong> 中，read() 是用來讀取資料，而 df.select() 是用來選擇需要的欄位</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT start_station_id, start_station_name, end_station_id, end_station_name
FROM NYC_CitiBike.DATE_HANDLING</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/3a337c2b834b3f7b.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df = df.select(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;)
save(df)
incorta.show(df)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/f4a299528b9df0fe.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="COUNT() vs count()" duration="0">
        <p>在 <strong>SQL</strong> 中，COUNT() 是用來計算有多少列的資料 (row numbers)</p>
<p>在 <strong>PySpark</strong> 中，count() 也是用來計算有多少列的資料 (row numbers)</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT COUNT(*) FROM NYC_CitiBike.DATE_HANDLING</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/a2509186097723d.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df.count()</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/16d91daf4bdb7e25.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="DISTINCT  vs  distinct(), dropDuplicates()" duration="0">
        <p>在 <strong>SQL</strong> 中，DISTINCT 是用來找出有多少列<strong>不同</strong>的資料，在 DISTINCT 後面加上欄位，及找出指定欄位的<strong>不同</strong>資料</p>
<p>在 <strong>PySpark</strong> 中，distinct() 也是用來計算有多少列<strong>不同</strong>的資料，但是 distinct() 只能用在比較所有的欄位時使用，而 dropDuplicates() 則是可以用來計算<strong>指定欄位</strong>有多少<strong>不同</strong>的資料</p>
<h2 is-upgraded><strong>Example - 單一欄位 - SQL</strong></h2>
<pre><code>%sql
SELECT COUNT(DISTINCT start_station_id)
FROM NYC_CitiBike.DATE_HANDLING</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/c8f7eead8d9bb509.png"></p>
<h2 is-upgraded><strong>Example - 單一欄位 - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.dropDuplicates([&#34;start_station_id&#34;]).count()
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/22ba2c62feaedc11.png"></p>
<h2 is-upgraded><strong>Example - 多欄位 - SQL</strong></h2>
<pre><code>%sql
SELECT COUNT(DISTINCT start_station_id, end_station_id)
FROM NYC_CitiBike.DATE_HANDLING</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/7a8b3d9cb0c6e46f.png"></p>
<h2 is-upgraded><strong>Example - 多欄位 - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.dropDuplicates([&#34;start_station_id&#34;,&#34;end_station_id&#34;]).count()
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/c6ac6129982d90c8.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="WHERE vs filter()" duration="0">
        <p>在 <strong>SQL</strong> 中，WHERE 是用來篩選特定條下的資料</p>
<p>在 <strong>PySpark</strong> 中，filter() 也是用來篩選特定條下的資料</p>
<p>但是在寫 <strong>SQL</strong> 時，會先寫 SELECT .. FROM .. 之後才是 WHERE ..</p>
<p>而在寫 <strong>PySpark</strong> 時，會先用 filter() 來篩選特定資料，才開始做其他運算</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/6b042dcc65327d1a.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df = df.select(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;)
df = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
save(df)
incorta.show(df)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/d11391e2f8fb0d63.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="GROUP BY vs groupBy()" duration="0">
        <p>在 <strong>SQL</strong> 中，GROUP BY 是搭配 aggregation function（聚合函數）使用，是用來將查詢結果中特定欄位值相同的資料分為若干個群組，而每一個群組都會傳回一個資料列</p>
<p>在 <strong>PySpark</strong> 中，groupBy() 也是一樣的，搭配 aggregation function（聚合函數）使用，是用來將查詢結果中特定欄位值相同的資料分為若干個群組，而每一個群組都會傳回一個資料列</p>
<p>常見的 aggregation function（聚合函數）有：avg()、count()、max()、min()、sum() 、mean()、... 等</p>
<h2 is-upgraded><strong>Example - 單一聚合函數 - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name, AVG(tripduration)
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/d94aed00872967dd.png"></p>
<h2 is-upgraded><strong>Example - 單一聚合函數 - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/333ccac7f2b94056.png"></p>
<h2 is-upgraded><strong>Example - 多聚合函數 - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name, 
        AVG(tripduration), SUM(tripduration), COUNT(tripduration)
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/1c250814e400a8f2.png"></p>
<h2 is-upgraded><strong>Example1 - 多聚合函數 - PySpark</strong></h2>
<pre><code>%pyspark
import pyspark.sql.functions as F
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;)\
         .agg(F.avg(&#34;tripduration&#34;),
              F.sum(&#34;tripduration&#34;),
              F.count(&#34;tripduration&#34;)
              )
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/2bd5bc97391b3b1.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="ORDER BY vs sort()" duration="0">
        <p>在 <strong>SQL</strong> 中，ORDER BY 可以根據指定欄位來排序</p>
<p>在 <strong>PySpark</strong> 中，sort() 也是用來根據指定欄位來排序</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name, AVG(tripduration)
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/8f474a1e5b39dbb3.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)
df1 = df1.sort(&#34;start_station_id&#34;, &#34;end_station_id&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/7c0085b072e56ac0.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="HAVING vs filter()" duration="0">
        <p>在 <strong>SQL</strong> 中，HAVING 是aggregate filter，用來篩選聚合函數所計算後的特定條下資料</p>
<p>在 <strong>PySpark</strong> 中，filter() 也是用來篩選特定條下的資料</p>
<p>但是在 <strong>SQL</strong> 中有固定的語序， SELECT → FROM → WHERE → GROUP BY →</p>
<p>HAVING → ORDER BY</p>
<p>在<strong>PySpark</strong> 中，語序可以自訂，因此只要利用 groupBy 計算後，再用 filter() 或 where() 篩選，即可得到同樣的結果</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT start_station_id, COUNT(*) start_station_count
FROM NYC_CitiBike.DATE_HANDLING
GROUP BY start_station_id
HAVING COUNT(*) &gt; 100</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/9a8d840ce185b614.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.groupBy(&#34;start_station_id&#34;).count().filter(&#34;count &gt; 100&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/b783a139e1066a76.png"></p>
<aside class="special"><p><strong>NOTE : </strong></p>
<p><strong>在 PySpark 中，若是先用 filter() 再使用 groupBy() 相當於 SQL 中的 WHERE ；</strong></p>
<p><strong>但若先用 groupBy() 再使用 filter() 就是相當於 SQL 中的 HAVING</strong></p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Alias" duration="0">
        <p>在 <strong>SQL</strong> 中，alias 可以在要重新命名的欄位後方加上 as [新的名字] 或是直接在要重新命名的欄位後方加上 [新的名字] </p>
<p>在 <strong>PySpark</strong> 中，withColumnRenamed() 是用來重新命名的函數</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id first_station_id, start_station_name first_station_name, end_station_id second_station_id, end_station_name second_station_name,
        AVG(tripduration) avg_by_first_to_second_station
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/858e3691ddd346d2.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)
df1 = df1.withColumnRenamed(&#34;start_station_id&#34;, &#34;first_station_id&#34;)\
         .withColumnRenamed(&#34;start_station_name&#34;, &#34;first_station_name&#34;)\
         .withColumnRenamed(&#34;end_station_id&#34;, &#34;second_station_id&#34;)\
         .withColumnRenamed(&#34;end_station_name&#34;, &#34;second_station_name&#34;)\
         .withColumnRenamed(&#34;avg(tripduration)&#34;, &#34;avg_by_first_to_second_station&#34;)\
         .sort(&#34;start_station_id&#34;, &#34;end_station_id&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/d066c39d39ca57c4.png"></p>
<aside class="special"><p><strong>TIP: </strong>在這個例子可以發現，PySpark 可以一次下很多個指令，用 <code>.</code> 連接，若是指令太長，也可以利用 <code>\</code> 加在一行的最後，再換行即可連續下指令</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="兩站之間的平均時間 - SQL" duration="0">
        <p>利用前面學習的步驟，可以計算 NYC CitiBike 自行車租借站每兩站之間的平均租借時間，之後還可以分別看從A站到B站的平均時間，和從B站到A站的平均時間，進行比較和探討差異的原因，此頁為利用 <strong>SQL</strong> 技巧運算</p>
<h2 is-upgraded><strong>Example - Average duration between two stations - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id first_station_id, start_station_name first_station_name, end_station_id second_station_id, end_station_name second_station_name,
        AVG(tripduration) avg_by_first_to_second_station
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/1d3cb90d1d0b0da5.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="兩站之間的平均時間 - spark.sql" duration="0">
        <p>利用前面學習的步驟，可以計算 NYC CitiBike 自行車租借站每兩站之間的平均租借時間，之後還可以分別看從A站到B站的平均時間，和從B站到A站的平均時間，進行比較和探討差異的原因，此頁為利用 <strong>pyspark 中的 spark.sql</strong> 技巧運算</p>
<h2 is-upgraded><strong>Example - Average duration between two stations - spark.sql</strong></h2>
<pre><code>%pyspark
df1 = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1.createOrReplaceTempView(&#34;DATE_H_1&#34;)
df2 = spark.sql(&#34;&#34;&#34;
SELECT  start_station_id first_station_id, start_station_name first_station_name, end_station_id second_station_id, end_station_name second_station_name,
        AVG(tripduration) avg_by_first_to_second_station
FROM DATE_H_1
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id
&#34;&#34;&#34;)
df2.createOrReplaceTempView(&#34;first_to_second&#34;)
incorta.show(df2)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/2f156ef70f72f64d.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="兩站之間的平均時間 - PySpark" duration="0">
        <p>利用前面學習的步驟，可以計算 NYC CitiBike 自行車租借站每兩站之間的平均租借時間，之後還可以分別看從A站到B站的平均時間，和從B站到A站的平均時間，進行比較和探討差異的原因，此頁為利用 <strong>PySpark</strong> 技巧運算</p>
<h2 is-upgraded><strong>Example - Average duration between two stations - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df3 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)\
        .groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)\
        .withColumnRenamed(&#34;start_station_id&#34;, &#34;first_station_id&#34;)\
        .withColumnRenamed(&#34;start_station_name&#34;, &#34;first_station_name&#34;)\
        .withColumnRenamed(&#34;end_station_id&#34;, &#34;second_station_id&#34;)\
        .withColumnRenamed(&#34;end_station_name&#34;, &#34;second_station_name&#34;)\
        .withColumnRenamed(&#34;avg(tripduration)&#34;, &#34;avg_by_first_to_second_station&#34;)\
        .sort(&#34;start_station_id&#34;, &#34;end_station_id&#34;)
save(df3)
incorta.show(df3)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/13604c04c23511f4.png"></p>
<p class="image-container"><img style="width: 601.70px" src="img/497a654d7273cf6f.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Table Data Comparison" duration="0">
        <p>可以利用 <code>exceptAll()</code> 函數比較任意兩個資料表間不同的地方，若是完全相同，則不會出現任何資料</p>
<h2 is-upgraded><strong>Example</strong></h2>
<p>利用 <code>exceptAll()</code> 查看上面兩種方法是否相同，結果得出，df2 和 df3 是完全相同的資料表，因此不會有資料在 <code>table_diff</code> 中</p>
<pre><code>%pyspark
import pyspark.sql. functions as F
df23_diff = df2.exceptAll(df3)
df23_diff.withColumn(&#34;type&#34;, F.lit(&#34;DF2 not in DF3&#34;))
df32_diff = df3.exceptAll(df2)
df32_diff.withColumn(&#34;type&#34;, F.lit(&#34;DF3 not in DF2&#34;))
table_diff = df23_diff.unionAll(df32_diff)
incorta.show(table_diff)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/3f71aa46de8e9916.png"></p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
