
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Transition from SQL to PySpark II</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="Transition_from_SQL_to_PySpark_II"
                  title="Transition from SQL to PySpark II"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Summary" duration="0">
        <p>這篇 Codelab 主要要比較三種不同語法之間的差異，分別是 <strong>SQL</strong> 、pyspark 中的 <strong>spark.sql()</strong> 和 <strong>PySpark</strong> 的方法，要運算出需要的分析結果，可以同時用三種不一樣的方法進行，後面會以同一個例子，用三種方法運算一遍，可以發現三種方法相異和相同之處</p>
<p>在 spark.sql() 中的函數，基本上和 SQL 一模一樣，因此在介紹函數時，主要以 <strong>SQL</strong> 和 <strong>PySpark</strong> 為比較重點，最後會再比較三種方法的完整寫法</p>


      </google-codelab-step>
    
      <google-codelab-step label="SELECT FROM vs read() &#43; df.select()" duration="0">
        <p>在 <strong>SQL</strong> 中，SELECT .. FROM 是用來讀取資料和選擇需要的欄位</p>
<p>在 <strong>PySpark</strong> 中，read() 是用來讀取資料，而 df.select() 是用來選擇需要的欄位</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT start_station_id, start_station_name, end_station_id, end_station_name
FROM NYC_CitiBike.DATE_HANDLING</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/cdeb0f61cd0c279f.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df = df.select(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;)
save(df)
incorta.show(df)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/f694f4c0c0b77fb6.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="COUNT() vs count()" duration="0">
        <p>在 <strong>SQL</strong> 中，COUNT() 是用來計算有多少列的資料 (row numbers)</p>
<p>在 <strong>PySpark</strong> 中，count() 也是用來計算有多少列的資料 (row numbers)</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT COUNT(*) FROM NYC_CitiBike.DATE_HANDLING</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/40f477d31a5e7ced.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df.count()</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/73b74aa525af6d69.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="WHERE vs filter()" duration="0">
        <p>在 <strong>SQL</strong> 中，WHERE 是用來篩選特定條下的資料</p>
<p>在 <strong>PySpark</strong> 中，filter() 也是用來篩選特定條下的資料</p>
<p>但是在寫 <strong>SQL</strong> 時，會先寫 SELECT .. FROM .. 之後才是 WHERE ..</p>
<p>而在寫 <strong>PySpark</strong> 時，會先用 filter() 來篩選特定資料，才開始做其他運算</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/2299a3b311940800.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df = df.select(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;)
df = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
save(df)
incorta.show(df)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/f451529f91c83784.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="GROUP BY vs groupBy()" duration="0">
        <p>在 <strong>SQL</strong> 中，GROUP BY 是搭配 aggregation function（聚合函數）使用，是用來將查詢結果中特定欄位值相同的資料分為若干個群組，而每一個群組都會傳回一個資料列</p>
<p>在 <strong>PySpark</strong> 中，groupBy() 也是一樣的，搭配 aggregation function（聚合函數）使用，是用來將查詢結果中特定欄位值相同的資料分為若干個群組，而每一個群組都會傳回一個資料列</p>
<p>常見的 aggregation function（聚合函數）有：avg()、count()、max()、min()、sum() 、mean()、... 等</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name, AVG(tripduration)
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/f301ff2f328806a2.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/3e9694393798a30a.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="ORDER BY vs sort()" duration="0">
        <p>在 <strong>SQL</strong> 中，ORDER BY 可以根據指定欄位來排序</p>
<p>在 <strong>PySpark</strong> 中，sort() 也是用來根據指定欄位來排序</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id, start_station_name, end_station_id, end_station_name, AVG(tripduration)
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/20863b12df83be76.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)
df1 = df1.sort(&#34;start_station_id&#34;, &#34;end_station_id&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/50b8f27abe1df322.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Alias" duration="0">
        <p>在 <strong>SQL</strong> 中，alias 可以在要重新命名的欄位後方加上 as [新的名字] 或是直接在要重新命名的欄位後方加上 [新的名字] </p>
<p>在 <strong>PySpark</strong> 中，withColumnRenamed() 是用來重新命名的函數</p>
<h2 is-upgraded><strong>Example - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id first_station_id, start_station_name first_station_name, end_station_id second_station_id, end_station_name second_station_name,
        AVG(tripduration) avg_by_first_to_second_station
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/888fc4bfd87e63d7.png"></p>
<h2 is-upgraded><strong>Example - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)
df1 = df1.groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)
df1 = df1.withColumnRenamed(&#34;start_station_id&#34;, &#34;first_station_id&#34;)\
         .withColumnRenamed(&#34;start_station_name&#34;, &#34;first_station_name&#34;)\
         .withColumnRenamed(&#34;end_station_id&#34;, &#34;second_station_id&#34;)\
         .withColumnRenamed(&#34;end_station_name&#34;, &#34;second_station_name&#34;)\
         .withColumnRenamed(&#34;avg(tripduration)&#34;, &#34;avg_by_first_to_second_station&#34;)\
         .sort(&#34;start_station_id&#34;, &#34;end_station_id&#34;)
save(df1)
incorta.show(df1)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/7bf844d0002d3f97.png"></p>
<aside class="special"><p><strong>TIP: </strong>在這個例子可以發現，PySpark 可以一次下很多個指令，用 <code>.</code> 連接，若是指令太長，也可以利用 <code>\</code> 加在一行的最後，再換行即可連續下指令</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="兩站之間的平均時間 - SQL" duration="0">
        <p>利用前面學習的步驟，可以計算 NYC CitiBike 自行車租借站每兩站之間的平均租借時間，之後還可以分別看從A站到B站的平均時間，和從B站到A站的平均時間，進行比較和探討差異的原因，此頁為利用 <strong>SQL</strong> 技巧運算</p>
<h2 is-upgraded><strong>Example - Average duration between two stations - SQL</strong></h2>
<pre><code>%sql
SELECT  start_station_id first_station_id, start_station_name first_station_name, end_station_id second_station_id, end_station_name second_station_name,
        AVG(tripduration) avg_by_first_to_second_station
FROM NYC_CitiBike.DATE_HANDLING
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/48a08b3f6e65fe8.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="兩站之間的平均時間 - spark.sql" duration="0">
        <p>利用前面學習的步驟，可以計算 NYC CitiBike 自行車租借站每兩站之間的平均租借時間，之後還可以分別看從A站到B站的平均時間，和從B站到A站的平均時間，進行比較和探討差異的原因，此頁為利用 <strong>pyspark 中的 spark.sql</strong> 技巧運算</p>
<h2 is-upgraded><strong>Example - Average duration between two stations - spark.sql</strong></h2>
<pre><code>%pyspark
df1 = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df1.createOrReplaceTempView(&#34;DATE_H_1&#34;)
df_2 = spark.sql(&#34;&#34;&#34;
SELECT  start_station_id first_station_id, start_station_name first_station_name, end_station_id second_station_id, end_station_name second_station_name,
        AVG(tripduration) avg_by_first_to_second_station
FROM DATE_H_1
WHERE start_station_id &lt;= end_station_id
GROUP BY start_station_id, start_station_name, end_station_id, end_station_name
ORDER BY start_station_id, end_station_id
&#34;&#34;&#34;)
df_2.createOrReplaceTempView(&#34;first_to_second&#34;)
incorta.show(df_2)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/68c35566fe735f95.png"></p>
<p class="image-container"><img style="width: 601.70px" src="img/6ef387bf17131f2e.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="兩站之間的平均時間 - PySpark" duration="0">
        <p>利用前面學習的步驟，可以計算 NYC CitiBike 自行車租借站每兩站之間的平均租借時間，之後還可以分別看從A站到B站的平均時間，和從B站到A站的平均時間，進行比較和探討差異的原因，此頁為利用 <strong>PySpark</strong> 技巧運算</p>
<h2 is-upgraded><strong>Example - Average duration between two stations - PySpark</strong></h2>
<pre><code>%pyspark
df = read(&#34;NYC_CitiBike.DATE_HANDLING&#34;)
df3 = df.filter(&#34;start_station_id &lt;= end_station_id&#34;)\
        .groupBy(&#34;start_station_id&#34;, &#34;start_station_name&#34;, &#34;end_station_id&#34;, &#34;end_station_name&#34;).avg(&#34;tripduration&#34;)\
        .withColumnRenamed(&#34;start_station_id&#34;, &#34;first_station_id&#34;)\
        .withColumnRenamed(&#34;start_station_name&#34;, &#34;first_station_name&#34;)\
        .withColumnRenamed(&#34;end_station_id&#34;, &#34;second_station_id&#34;)\
        .withColumnRenamed(&#34;end_station_name&#34;, &#34;second_station_name&#34;)\
        .withColumnRenamed(&#34;avg(tripduration)&#34;, &#34;avg_by_first_to_second_station&#34;)\
        .sort(&#34;start_station_id&#34;, &#34;end_station_id&#34;)
save(df3)
incorta.show(df3)</code></pre>
<p class="image-container"><img style="width: 601.70px" src="img/f474d3470d9ea85c.png"></p>
<p class="image-container"><img style="width: 601.70px" src="img/3b230837e679218e.png"></p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
